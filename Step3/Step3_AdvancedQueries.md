## Query 1: 
- Query: O365
- URL: https://trainingcataloguesearch.search.windows.net/indexes/courses-indexv2/docs?api-version=2021-04-30-Preview&search=O365
- Result:
```json
{
  "@odata.context": "https://trainingcataloguesearch.search.windows.net/indexes('courses-indexv2')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 28.206577,
      "Key": "Y29tcGFueS1tb29kbGUxN2IxZWVkYy0wZTk2LTRlNWItODE5OS04M2E0ODQzODhlZmU1",
      "source": "Company Moodle",
      "title": "O365",
      "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues",
      "instructor": "Gerald Dominguez",
      "level": "beginner",
      "role": "all",
      "product": "O365",
      "duration": 2,
      "rating_count": 510,
      "rating_average": 4.6,
      "url": "https://www.example.com/course10",
      "keyphrases": [
        "internal best practices",
        "O365 suite",
        "email signatures",
        "file storage",
        "other issues"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Gerald Dominguez\",\"description\":\"Gerald is a Junior IT Administrator and has been with the company for 1 year. Gerald played NCAA soccer and is a part-time soccer coach on weekends. \",\"matches\":[{\"text\":\"gerald dominguez\",\"offset\":0,\"length\":16,\"matchDistance\":0.0}]}]"
    }
  ]
}
```
------------------------------------------------

## Query 2:
- Query: Eileen Diaz
- URL: https://trainingcataloguesearch.search.windows.net/indexes/courses-indexv2/docs?api-version=2021-04-30-Preview&search=Eileen%20Diaz
- Result:
```json
{
  "@odata.context": "https://trainingcataloguesearch.search.windows.net/indexes('courses-indexv2')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 2.7090158,
      "Key": "Y29tcGFueS1tb29kbGUyNzhkMjk5ZS1lZjBlLTQ3ZmItOGU5OC01YTMxYTA3MzUxOWM1",
      "source": "Company Moodle",
      "title": "Security for database code",
      "description": "For developers, learn our best practices for securely connecting to databases",
      "instructor": "Eileen Diaz",
      "level": "advanced",
      "role": "developer",
      "product": "SQL",
      "duration": 2,
      "rating_count": 115,
      "rating_average": 4.8,
      "url": "https://www.example.com/course7",
      "keyphrases": [
        "best practices",
        "developers",
        "databases"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 2.7090158,
      "Key": "Y29tcGFueS1tb29kbGU2YjVkM2Y1NS1lYjAyLTQ5OWQtOTc3NS0yZTBlMjU2NTllMDc1",
      "source": "Company Moodle",
      "title": "Ethics in AI",
      "description": "Learn our company's Principles for the Responsible Use of AI",
      "instructor": "Eileen Diaz",
      "level": "intermediate",
      "role": "architect",
      "product": "NA",
      "duration": 1,
      "rating_count": 24,
      "rating_average": 4.3,
      "url": "https://www.example.com/course12",
      "keyphrases": [
        "Responsible Use",
        "company",
        "Principles",
        "AI"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 2.7090158,
      "Key": "Y29tcGFueS1tb29kbGU4NWVlNzI1Yi00YWUwLTQ3MTktODc4NS1kZGY5OWUxOWZhZjE1",
      "source": "Company Moodle",
      "title": "Security for database admins",
      "description": "For administrators, learn our best practices for securing all databases",
      "instructor": "Eileen Diaz",
      "level": "advanced",
      "role": "admin",
      "product": "SQL",
      "duration": 3,
      "rating_count": 45,
      "rating_average": 4.3,
      "url": "https://www.example.com/course8",
      "keyphrases": [
        "best practices",
        "administrators",
        "databases"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 2.7090158,
      "Key": "Y29tcGFueS1tb29kbGU5ZGY4NDRmZC1lZWZlLTQ4ODAtODM0MS05MzY3MzIxNzRiYjU1",
      "source": "Company Moodle",
      "title": "Encryption and security",
      "description": "Learn our policies for utilizing encryption including key management for projects",
      "instructor": "Eileen Diaz",
      "level": "advanced",
      "role": "architect",
      "product": "NA",
      "duration": 3,
      "rating_count": 95,
      "rating_average": 4.2,
      "url": "https://www.example.com/course14",
      "keyphrases": [
        "key management",
        "policies",
        "encryption",
        "projects"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 2.7090158,
      "Key": "Y29tcGFueS1tb29kbGVkM2YwYzk1NS1hYzZlLTRjZWQtYjkxYi1mZmNlZjNlOGNlZGU1",
      "source": "Company Moodle",
      "title": "Code security",
      "description": "For developers, learn our best practices for writing secure code for web, server, and desktop development",
      "instructor": "Eileen Diaz",
      "level": "intermediate",
      "role": "developer",
      "product": "NA",
      "duration": 3,
      "rating_count": 132,
      "rating_average": 4.4,
      "url": "https://www.example.com/course9",
      "keyphrases": [
        "best practices",
        "secure code",
        "desktop development",
        "developers",
        "web",
        "server"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    }
  ]
}
```
-----------------------------------------

## Query 3:
- Query: $filter=instructor ne ''&$top=5&facet=instructor
- URL: https://trainingcataloguesearch.search.windows.net/indexes/courses-indexv2/docs?api-version=2021-04-30-Preview&search=*&%24filter=instructor%20ne%20''&%24top=5&facet=instructor
- Result:
```json
{
  "@odata.context": "https://trainingcataloguesearch.search.windows.net/indexes('courses-indexv2')/$metadata#docs(*)",
  "@search.facets": {
    "instructor": [
      {
        "count": 5,
        "value": "Eileen Diaz"
      },
      {
        "count": 3,
        "value": "Claudia Blackman"
      },
      {
        "count": 3,
        "value": "Mike Montoya"
      },
      {
        "count": 2,
        "value": "Gerald Dominguez"
      },
      {
        "count": 1,
        "value": "Robert Gillis"
      }
    ]
  },
  "value": [
    {
      "@search.score": 1,
      "Key": "Y29tcGFueS1tb29kbGUxN2IxZWVkYy0wZTk2LTRlNWItODE5OS04M2E0ODQzODhlZmU1",
      "source": "Company Moodle",
      "title": "O365",
      "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues",
      "instructor": "Gerald Dominguez",
      "level": "beginner",
      "role": "all",
      "product": "O365",
      "duration": 2,
      "rating_count": 510,
      "rating_average": 4.6,
      "url": "https://www.example.com/course10",
      "keyphrases": [
        "internal best practices",
        "O365 suite",
        "email signatures",
        "file storage",
        "other issues"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Gerald Dominguez\",\"description\":\"Gerald is a Junior IT Administrator and has been with the company for 1 year. Gerald played NCAA soccer and is a part-time soccer coach on weekends. \",\"matches\":[{\"text\":\"gerald dominguez\",\"offset\":0,\"length\":16,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 1,
      "Key": "Y29tcGFueS1tb29kbGUyNzhkMjk5ZS1lZjBlLTQ3ZmItOGU5OC01YTMxYTA3MzUxOWM1",
      "source": "Company Moodle",
      "title": "Security for database code",
      "description": "For developers, learn our best practices for securely connecting to databases",
      "instructor": "Eileen Diaz",
      "level": "advanced",
      "role": "developer",
      "product": "SQL",
      "duration": 2,
      "rating_count": 115,
      "rating_average": 4.8,
      "url": "https://www.example.com/course7",
      "keyphrases": [
        "best practices",
        "developers",
        "databases"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Eileen Diaz\",\"description\":\"Eileen is our Senior Security Engineer responsible for application and service security. She has been with the company for 9 years and enjoys writing Sci-Fi in her spare time.\",\"matches\":[{\"text\":\"eileen diaz\",\"offset\":0,\"length\":11,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 1,
      "Key": "Y29tcGFueS1tb29kbGUzMGUzYzZlNS05NDE1LTRkODUtODIyOS1jMjEzMzIwM2M1MzU1",
      "source": "Company Moodle",
      "title": "Onboarding - Technology Policies ",
      "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology",
      "instructor": "Mike Montoya",
      "level": "beginner",
      "role": "all",
      "product": "NA",
      "duration": 1,
      "rating_count": 550,
      "rating_average": 4.9,
      "url": "https://www.example.com/course2",
      "keyphrases": [
        "other technology",
        "policies",
        "distribution",
        "use",
        "computers",
        "phones",
        "software"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Mike Montoya\",\"description\":\"Mike  is our HR trainer responsible for helping employees be successful in their careers at our company.  He has been with us for 3 years. Mike is an amateur chef and enjoys outdoor live music events.\",\"matches\":[{\"text\":\"mike montoya\",\"offset\":0,\"length\":12,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 1,
      "Key": "Y29tcGFueS1tb29kbGUzY2JiMzgwMC0yNTU0LTQxMjEtYmIzYS1mMzY1ZGViMGMzYjY1",
      "source": "Company Moodle",
      "title": "Maps",
      "description": "Learn our best practices for various tools such as Leaflet",
      "instructor": "Robert Gillis",
      "level": "intermediate",
      "role": "developer",
      "product": "leaflet",
      "duration": 2,
      "rating_count": 28,
      "rating_average": 3.9,
      "url": "https://www.example.com/course6",
      "keyphrases": [
        "best practices",
        "various tools",
        "Leaflet"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Robert Gillis\",\"description\":\"Robert is our GIS specialist in charge of making beautiful maps for our users. He has been with the company for 8 years and enjoys hiking, fishing, and any other outdoor activity.\",\"matches\":[{\"text\":\"robert gillis\",\"offset\":0,\"length\":13,\"matchDistance\":0.0}]}]"
    },
    {
      "@search.score": 1,
      "Key": "Y29tcGFueS1tb29kbGU1NzhhMzMxOS1hYTdjLTRkMmYtYjZhNC0zOWU5NjM4YjBhODU1",
      "source": "Company Moodle",
      "title": "DevOps for Ops",
      "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective",
      "instructor": "Claudia Blackman",
      "level": "intermediate",
      "role": "admin",
      "product": "jenkins",
      "duration": 5,
      "rating_count": 56,
      "rating_average": 4.9,
      "url": "https://www.example.com/course5",
      "keyphrases": [
        "CI/CD pipelines",
        "operations perspective",
        "administrators",
        "course"
      ],
      "instructorDescription": null,
      "entities": null,
      "instructorEntities": "[{\"name\":\"Claudia Blackman\",\"description\":\"Claudia is our senior DevOps engineer. She is charged with overseeing our DevOps operations and has been with the company for 2 years. Claudia enjoys downhill skiing and is a member of the local Search & Rescue Team.\",\"matches\":[{\"text\":\"claudia blackman\",\"offset\":0,\"length\":16,\"matchDistance\":0.0}]}]"
    }
  ]
}
```
------------------------------------------------

## Query 4:
- Query: "Technological advancements and opportunities in Neuromarketing: a systematic review"&$select=metadata_title,publisher,publicationDate,DOI,publicationName
- URL: https://trainingcataloguesearch.search.windows.net/indexes/papers-index/docs?api-version=2021-04-30-Preview&search=%22Technological%20advancements%20and%20opportunities%20in%20Neuromarketing%3A%20a%20systematic%20review%22&%24select=metadata_title%2Cpublisher%2CpublicationDate%2CDOI%2CpublicationName
- Result:
```json
{
  "@odata.context": "https://trainingcataloguesearch.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 25.597557,
      "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "publicationName": "Brain Informatics",
      "publisher": "Springer",
      "DOI": "10.1186/s40708-020-00109-x",
      "publicationDate": "2020-09-21"
    }
  ]
}
```
------------------------------------------------

## Query 5:
- Query: network&facet=publisher&$top=3
- URL: https://trainingcataloguesearch.search.windows.net/indexes/papers-index/docs?api-version=2021-04-30-Preview&search=network&facet=publisher&%24top=3
```json
{
  "@odata.context": "https://trainingcataloguesearch.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "@search.facets": {
    "publisher": [
      {
        "count": 14,
        "value": "Springer"
      },
      {
        "count": 4,
        "value": ""
      }
    ]
  },
  "value": [
    {
      "@search.score": 4.3870344,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2d1ZXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L3BhcGVycy9zNDExMDktMDIwLTAwMzMwLXgucGRm0",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "metadata_creation_date": "2020-11-12T15:20:34Z",
      "author": [
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "Anderson",
        "Kodate",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "Akoglu",
        "Pu",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "Savage",
        "Šubelj",
        "Dreżewski",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "Van Vlasselaer",
        "Bhat",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Van Vlas",
        "selaer",
        "Li",
        "Monamo",
        "Ferrara",
        "Hu",
        "Yan",
        "ini",
        "kouti",
        "vi",
        "vj",
        "sini",
        "souti",
        "Palla",
        "Milo",
        "Breiman",
        "Hastie",
        "Pedregosa",
        "Strobl",
        "Altmann",
        "Ci"
      ],
      "institutions": [
        "UK Parliament",
        "McAfee",
        "Commons",
        "iveco",
        "University",
        "Google LLC",
        "eBay",
        "Mercari",
        "Colladon",
        "WSPi",
        "Newman",
        "CYPi",
        "si/ki"
      ],
      "phrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "illegal items",
        "fictive items",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "iveco mmons",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "doi.org",
        "orcid.org",
        "Fraud detection",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "naokimas",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "creativecommons",
        "licenses",
        "crossmark",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "one jargon",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "informa- tion",
        "users’ network",
        "fictive item",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "mean number",
        "focal user",
        "Fictive transactions",
        "directed edge",
        "networks",
        "global",
        "communities",
        "betweenness",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "hygiene",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "1a",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci ranges",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "one item",
        "triangle congregation",
        "other four",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "ki(ki",
        "Fig. 2c",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Ciki",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "Breiman",
        "The precision–recall (PR) curve",
        "random forest clas- sifier",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "Directed triangle patterns",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "three-node patterns",
        "directed triangles",
        "same order",
        "single-tree positive",
        "Cyclic triangle",
        "tive samples",
        "precision range",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "Feedforward triangle",
        "decision trees",
        "300 trees",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "degree distribution",
        "unweighted degree",
        "user type",
        "Fig. 3a",
        "Fig. 3c",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "Fig. 4a",
        "Fig.  5a",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "ki",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "Figure  5b",
        "dashed lines",
        "Figure 5b",
        "two aspects",
        "Figure  6b",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "second feature",
        "Fig.  6a",
        "Fig. 8a",
        "relationships",
        "ini",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "unweighted sell probability SPi",
        "Fig. 8 Cycle probability",
        "Fig. 7 Triangle congregation",
        "b Survival probability",
        "third feature",
        "real number",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "c Relationship",
        "WSPi"
      ],
      "publicationName": "Applied Network Science",
      "publisher": "Springer",
      "DOI": "10.1007/s41109-020-00330-x",
      "publicationDate": "2020-11-16"
    },
    {
      "@search.score": 3.6724787,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-s",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2d1ZXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L3BhcGVycy9zMTM2NDAtMDIwLTAwNTQ1LXoucGRm0",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "author": [
        "Haoliang Cui1,",
        "Shuai Shao2",
        "Shaozhang Niu1,",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Bengio",
        "Kim",
        "Liu",
        "Peters",
        "Devlin",
        "Wu",
        "Bayes",
        "Malli",
        "Chen",
        "Ning",
        "Yin",
        "Binder",
        "Jieba",
        "thon"
      ],
      "institutions": [
        "Security Evaluation Center",
        "BERT",
        "Taobao",
        "EURASIP Journal",
        "EURASIP",
        "Tencent AI Lab",
        "CNN",
        "RNN",
        "ELMO",
        "curation social network",
        "CCSN",
        "ule",
        "Intelligent Space",
        "AMS",
        "PMS",
        "Binder IPC",
        "SCRM",
        "activity manager service",
        "process",
        "JD",
        "JD.COM",
        "TF",
        "IDF",
        "TF-IDF",
        "Scikit-Learn",
        "TFIDF",
        "Unicode",
        "MLM",
        "Google"
      ],
      "phrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "dividend",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ],
      "publicationName": "EURASIP Journal on Image and Video Processing",
      "publisher": "Springer",
      "DOI": "10.1186/s13640-020-00545-z",
      "publicationDate": "2021-01-14"
    },
    {
      "@search.score": 2.079117,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2d1ZXN0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L3BhcGVycy9zNDA1MzctMDE5LTAxODQtNS5wZGY1",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tu Nguyen Thi Ngoc ",
      "metadata_title": "Mining aspects of customer’s review on the social network",
      "metadata_creation_date": "2019-02-26T14:27:28Z",
      "author": [
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Viet Anh Nguyen2",
        "Naive",
        "Nguyen Thi Ngoc",
        "Thi Ngoc",
        "Trung Nguyen",
        "Dark",
        "Soft",
        "Hu",
        "Liu",
        "eling",
        "Lin",
        "Pang",
        "Lee",
        "Moghaddam",
        "Ester",
        "Xiaowen Ding",
        "Minqing Hu",
        "Yan",
        "Peñalver-Martinez",
        "Naïve Bayes",
        "Asha",
        "Gini",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "K",
        "Bayes",
        "fk"
      ],
      "institutions": [
        "Bayes",
        "Commons",
        "Electric Power University",
        "MYOB",
        "LDA",
        "FLDA",
        "TripAdvisor",
        "Support",
        "JJ",
        "NN",
        "RB",
        "fk"
      ],
      "phrases": [
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "creat iveco mmons",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "cus- tomer reviews",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "doi.org",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "METHODOLOGY",
        "Correspondence",
        "tunn",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "21Nguyen Thi Ngoc",
        "Three tasks Extracting",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "manu- ally",
        "main challenge",
        "many reviews",
        "big fan",
        "thorough understanding",
        "general impression",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "two important tasks",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "many researches work",
        "aspect weighting tasks",
        "Different approach",
        "earliest researches",
        "Related work",
        "top pot",
        "condensed milk",
        "supervised approach",
        "sophisticated state",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "Method” sections",
        "frequency threshold",
        "frequency-based approach",
        "aspect extracting",
        "aspect identification",
        "aspect terms",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "frequency-based method",
        "regression methods",
        "interesting methods",
        "words frequency",
        "chocolate",
        "note",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "reviews",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "occurrence",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "topic- modeling approaches",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "rule-based approaches",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "fac",
        "item",
        "work",
        "fication",
        "sampling",
        "extraction",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "wk| k",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "intensification",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "R K",
        "K-dimensional vector",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "aspect aj",
        "vector ri",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "rmin",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1,Q",
        "yi",
        "wk",
        "corpus",
        "Sj",
        "Tj",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "occurrences",
        "frequency",
        "subset",
        "initial aspect core terms",
        "flavor taste aftertaste mouthfeel",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "aspect weight",
        "Aspect ratings",
        "new words",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "weighted sum",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "JJ tags",
        "NN tags",
        "RB tags",
        "VB tags",
        "Laplace transformation",
        "class c",
        "one word",
        "first word",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "POS tags",
        "P(rij",
        "class label",
        "second word",
        "rating label",
        "rating rij",
        "rijαij",
        "third rules",
        "feature fk",
        "word features",
        "probability rij",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "POS labeled rules",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "third word",
        "JJ NN",
        "NNS JJ",
        "fq",
        "∏q",
        "fk",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method"
      ],
      "publicationName": "Journal of Big Data",
      "publisher": "Springer",
      "DOI": "10.1186/s40537-019-0184-5",
      "publicationDate": "2019-02-28"
    }
  ]
}
```